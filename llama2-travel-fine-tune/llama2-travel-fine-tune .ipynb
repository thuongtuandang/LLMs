{"cells":[{"cell_type":"markdown","metadata":{},"source":["This is a project to fine-tune Llama 2 for a travel chatbox. This notebook can be run in kaggle. Together with Phuoc Bui, we collected data from reddit for the task. The dataset reddit-more.csv consists of more than 8000 question-answer pairs from several subreddits related to travel.\n","\n","Here are steps to fine-tune Llama 2 (or any LLM) with your own dataset:\n","\n","Step 0.\n","- Install and import libraries: this sounds easy, but in some cases, it can be complicated due to the compatibility, versions, ...\n","\n","Step 1.\n","- Bits and bytes configuration for quantization.\n","- Define the model with model_name and quantization_config.\n","- Define the tokenizer with model_name.\n","\n","Step 2.\n","- Overview of this LLM, see how many layers and parameters it has,\n","- Get information about the last layer to fine-tune.\n","\n","Step 3.\n","- Define lora_config with parameters r, lora_alpha, etc...\n","- Redefine the model with that lora config: this will freeze parameters of previous layers, and we update only parameters for the last layer.\n","\n","Step 4.\n","- Define generation_config: this is for the result generation with parameters: max_new_tokens, temperature, top_p, etc..\n","\n","Step 5.\n","- Test pretrained model by defining a prompt with a question-answer pair. \n","- Define encoding = tokenizer(prompt)\n","- Generate output = model.generate(encoding)\n","\n","Step 6.\n","- Define data for our LLM. Note that all question-answer pairs will be tokenized in this step.\n","\n","Step 7.\n","- Define training_args with some parameters such as output_dir, learning rate, optimizer, ...\n","- Define trainer = transformer(model, data, training_args)\n","- Fine tune the model by calling trainer.train()\n","\n","Step 8.\n","- Evaluate model on test set with an NLP metric.\n","\n","Step 9.\n","- Save the pretrained model.\n","\n","A very good reference can be found here by PHIL CULLITON: https://www.kaggle.com/code/philculliton/fine-tuning-with-llama-2-qlora"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.155800Z","iopub.status.busy":"2024-01-05T12:57:23.154669Z","iopub.status.idle":"2024-01-05T12:57:23.169310Z","shell.execute_reply":"2024-01-05T12:57:23.167949Z","shell.execute_reply.started":"2024-01-05T12:57:23.155755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/llama-2/pytorch/13b-chat-hf/1/model.safetensors.index.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/model-00003-of-00003.safetensors\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/config.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/pytorch_model-00003-of-00003.bin\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/Responsible-Use-Guide.pdf\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/pytorch_model-00002-of-00003.bin\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/README.md\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/USE_POLICY.md\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/tokenizer.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/model-00001-of-00003.safetensors\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/tokenizer_config.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/pytorch_model.bin.index.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/LICENSE.txt\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/model-00002-of-00003.safetensors\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/special_tokens_map.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/.gitattributes\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/pytorch_model-00001-of-00003.bin\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/tokenizer.model\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/added_tokens.json\n","/kaggle/input/llama-2/pytorch/13b-chat-hf/1/generation_config.json\n","/kaggle/input/reddit-more/reddit_more.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["We will take a look on our dataset. Because we have limited resources, our data frame will consist only 20 rows for fine-tuning."]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.171899Z","iopub.status.busy":"2024-01-05T12:57:23.171284Z","iopub.status.idle":"2024-01-05T12:57:23.470543Z","shell.execute_reply":"2024-01-05T12:57:23.469456Z","shell.execute_reply.started":"2024-01-05T12:57:23.171865Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I went on ‘the lost city’ trek using G Adventu...</td>\n","      <td>Tbh this applies to any tour site\\n\\nI look fo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Thought I'd pass along the information and adv...</td>\n","      <td>Try disputing it with your credit company</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I made this [post](https://www.reddit.com/r/Tr...</td>\n","      <td>I’m so glad to hear you made it safely!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Here's the deal. The other day, I booked a tri...</td>\n","      <td>Which airline did you book with? Some airlines...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hi all! I’m visiting NYC  this month and I wou...</td>\n","      <td>Eat pizza slices.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>USA is such an interesting place, if I were ri...</td>\n","      <td>I try to warn inexperienced travelers in US tr...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Hey guys as the title mentions I’ll be traveli...</td>\n","      <td>Packing less products / clothes - packing more...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I’ll be traveling with a 1 1/2 year old this f...</td>\n","      <td>I used to roll my eyes at folks who put leashe...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>I’ve been told that it’s not a great idea to f...</td>\n","      <td>I'm a retired Flight Paramedic who flew in hel...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Am I shit out of luck? I’m so choked that our ...</td>\n","      <td>Who changed the itinerary? If it is the airlin...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  \\\n","0  I went on ‘the lost city’ trek using G Adventu...   \n","1  Thought I'd pass along the information and adv...   \n","2  I made this [post](https://www.reddit.com/r/Tr...   \n","3  Here's the deal. The other day, I booked a tri...   \n","4  Hi all! I’m visiting NYC  this month and I wou...   \n","5  USA is such an interesting place, if I were ri...   \n","6  Hey guys as the title mentions I’ll be traveli...   \n","7  I’ll be traveling with a 1 1/2 year old this f...   \n","8  I’ve been told that it’s not a great idea to f...   \n","9  Am I shit out of luck? I’m so choked that our ...   \n","\n","                                              Answer  \n","0  Tbh this applies to any tour site\\n\\nI look fo...  \n","1          Try disputing it with your credit company  \n","2            I’m so glad to hear you made it safely!  \n","3  Which airline did you book with? Some airlines...  \n","4                                  Eat pizza slices.  \n","5  I try to warn inexperienced travelers in US tr...  \n","6  Packing less products / clothes - packing more...  \n","7  I used to roll my eyes at folks who put leashe...  \n","8  I'm a retired Flight Paramedic who flew in hel...  \n","9  Who changed the itinerary? If it is the airlin...  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('/kaggle/input/reddit-more/reddit_more.csv')\n","df = df.iloc[150:171]\n","df.reset_index(drop=True, inplace=True)\n","df.iloc[:10]"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.472795Z","iopub.status.busy":"2024-01-05T12:57:23.471918Z","iopub.status.idle":"2024-01-05T12:57:23.483080Z","shell.execute_reply":"2024-01-05T12:57:23.481658Z","shell.execute_reply.started":"2024-01-05T12:57:23.472749Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question    0\n","Answer      0\n","dtype: int64\n","Question    0\n","Answer      0\n","dtype: int64\n"]}],"source":["# Remove NaN\n","print(df.isna().sum())\n","df = df.dropna()\n","print(df.isna().sum())"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.486101Z","iopub.status.busy":"2024-01-05T12:57:23.485764Z","iopub.status.idle":"2024-01-05T12:57:23.495679Z","shell.execute_reply":"2024-01-05T12:57:23.494695Z","shell.execute_reply.started":"2024-01-05T12:57:23.486074Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['Going on my first red eye, international flight soon. I’ve only ever done short domestic flights. And I’ve never used the bathroom on a plane. \\n\\n\\nI heard someone say that if you’re in a window seat you should try and use the bathroom when the person sitting next to you gets up to use the bathroom. \\n\\n\\nBut other than that, if there any general etiquette that I should be aware of when in the window seat? \\n\\n\\nEspecially when it comes to possibly having to wake someone up to use the bathroom.',\n","       'I’ve been looking for an app that could track which countries/cities I’ve been to, like an interactive map of some sort.\\n\\n\\nIs there a good app for doing this and gradually updating it? I think it’d be pretty fun!\\n\\n\\nEdit: I know about google maps. Looking for something more separate from my usual stuff',\n","       'Top of the list rn is most likely Italy, just need to decide on a city. Probably going around late June, looking for some place just to chill with lots of night life, a nice city centre, and stuff to explore for 3/4 days.  Thank you!'],\n","      dtype=object)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["df[\"Question\"].values[10:13]"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.497149Z","iopub.status.busy":"2024-01-05T12:57:23.496843Z","iopub.status.idle":"2024-01-05T12:57:23.505248Z","shell.execute_reply":"2024-01-05T12:57:23.504376Z","shell.execute_reply.started":"2024-01-05T12:57:23.497124Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Going on my first red eye, international flight soon. I’ve only ever done short domestic flights. And I’ve never used the bathroom on a plane. \\n\\n\\nI heard someone say that if you’re in a window seat you should try and use the bathroom when the person sitting next to you gets up to use the bathroom. \\n\\n\\nBut other than that, if there any general etiquette that I should be aware of when in the window seat? \\n\\n\\nEspecially when it comes to possibly having to wake someone up to use the bathroom.'"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["df[\"Question\"].values[10]"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.506758Z","iopub.status.busy":"2024-01-05T12:57:23.506382Z","iopub.status.idle":"2024-01-05T12:57:23.514599Z","shell.execute_reply":"2024-01-05T12:57:23.513653Z","shell.execute_reply.started":"2024-01-05T12:57:23.506727Z"},"trusted":true},"outputs":[],"source":["# Remove \\n\n","df['Question'] = df['Question'].str.replace('\\n', '')\n","df['Answer'] = df['Answer'].str.replace('\\n', '')"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.516164Z","iopub.status.busy":"2024-01-05T12:57:23.515825Z","iopub.status.idle":"2024-01-05T12:57:23.525090Z","shell.execute_reply":"2024-01-05T12:57:23.523866Z","shell.execute_reply.started":"2024-01-05T12:57:23.516131Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['Going on my first red eye, international flight soon. I’ve only ever done short domestic flights. And I’ve never used the bathroom on a plane. I heard someone say that if you’re in a window seat you should try and use the bathroom when the person sitting next to you gets up to use the bathroom. But other than that, if there any general etiquette that I should be aware of when in the window seat? Especially when it comes to possibly having to wake someone up to use the bathroom.',\n","       'I’ve been looking for an app that could track which countries/cities I’ve been to, like an interactive map of some sort.Is there a good app for doing this and gradually updating it? I think it’d be pretty fun!Edit: I know about google maps. Looking for something more separate from my usual stuff',\n","       'Top of the list rn is most likely Italy, just need to decide on a city. Probably going around late June, looking for some place just to chill with lots of night life, a nice city centre, and stuff to explore for 3/4 days.  Thank you!'],\n","      dtype=object)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# Check\n","df['Question'].values[10:13]"]},{"cell_type":"markdown","metadata":{},"source":["Step 0.\n","- Install and import libraries"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:57:23.526367Z","iopub.status.busy":"2024-01-05T12:57:23.526073Z","iopub.status.idle":"2024-01-05T12:59:47.048473Z","shell.execute_reply":"2024-01-05T12:59:47.047330Z","shell.execute_reply.started":"2024-01-05T12:57:23.526324Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!pip install -qqq bitsandbytes\n","!pip install -qqq torch\n","!pip install  -qqq -U git+https://github.com/huggingface/transformers\n","!pip install -qqq -U git+https://github.com/huggingface/peft\n","!pip install -qqq -U git+https://github.com/huggingface/accelerate\n","!pip install -qqq datasets\n","!pip install -qqq loralib\n","!pip install -qqq einops"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T12:59:47.052853Z","iopub.status.busy":"2024-01-05T12:59:47.052491Z","iopub.status.idle":"2024-01-05T12:59:47.060303Z","shell.execute_reply":"2024-01-05T12:59:47.059213Z","shell.execute_reply.started":"2024-01-05T12:59:47.052819Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","from pprint import pprint\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import load_dataset, Dataset\n","from huggingface_hub import notebook_login\n","\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""]},{"cell_type":"markdown","metadata":{},"source":["Step 1.\n","- Bits and bytes configuration for quantization.\n","- Define the model with model_name and quantization_config.\n","- Define the tokenizer with model_name."]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:12:46.419161Z","iopub.status.busy":"2024-01-05T13:12:46.418773Z","iopub.status.idle":"2024-01-05T13:12:57.242968Z","shell.execute_reply":"2024-01-05T13:12:57.242157Z","shell.execute_reply.started":"2024-01-05T13:12:46.419130Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ca085b245a9464c9435619ffa898c5b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n","MODEL_NAME = model\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config\n",")\n","\n","# PEFT wrapper the model for training / fine-tuning\n","model = prepare_model_for_kbit_training(model)\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{},"source":["Step 2.\n","- Overview of this LLM, see how many layers and parameters it has,\n","- Get information about the last layer to fine-tune."]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:13:07.205708Z","iopub.status.busy":"2024-01-05T13:13:07.205307Z","iopub.status.idle":"2024-01-05T13:13:07.214397Z","shell.execute_reply":"2024-01-05T13:13:07.213277Z","shell.execute_reply.started":"2024-01-05T13:13:07.205676Z"},"trusted":true},"outputs":[],"source":["import re\n","# This is to get the numer of layers of our LLM\n","def get_num_layers(model):\n","    # We first define a set\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        # Name is of this form: model.layers.2.post_attention_layernorm.weight\n","        # The number 2 means the index of the post attention layer norm\n","        # We use regular expression to parse the number 2\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    # The number of layers is exactly maximum value of the set numbers\n","    return max(numbers)\n","\n","# This is to get the number of parameters of our LLM\n","def get_num_params(model):\n","    num_params = 0\n","    for _, param in model.named_parameters():\n","        num_params += param.numel()\n","    return num_params\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    return names"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:03:52.481321Z","iopub.status.busy":"2024-01-05T13:03:52.480987Z","iopub.status.idle":"2024-01-05T13:03:52.496473Z","shell.execute_reply":"2024-01-05T13:03:52.495570Z","shell.execute_reply.started":"2024-01-05T13:03:52.481296Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['2']"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# Test\n","name = \"model.layers.2.post_attention_layernorm.weight\"\n","number = re.findall(r'\\d+', name)\n","number"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:03:52.497982Z","iopub.status.busy":"2024-01-05T13:03:52.497625Z","iopub.status.idle":"2024-01-05T13:03:52.517332Z","shell.execute_reply":"2024-01-05T13:03:52.516226Z","shell.execute_reply.started":"2024-01-05T13:03:52.497946Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["39\n","6671979520\n","['model.layers.39.self_attn.q_proj', 'model.layers.39.self_attn.k_proj', 'model.layers.39.self_attn.v_proj', 'model.layers.39.self_attn.o_proj', 'model.layers.39.mlp.gate_proj', 'model.layers.39.mlp.up_proj', 'model.layers.39.mlp.down_proj']\n"]}],"source":["# Total number of layers and parameters\n","print(get_num_layers(model))\n","# Total number of named params\n","print(get_num_params(model))\n","print(get_last_layer_linears(model))"]},{"cell_type":"markdown","metadata":{},"source":["Step 3.\n","- Define lora_config with parameters r, lora_alpha, etc...\n","- Redefine the model with that lora config: this will freeze parameters of previous layers, and we update only parameters for the last layer."]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:13:22.528792Z","iopub.status.busy":"2024-01-05T13:13:22.528398Z","iopub.status.idle":"2024-01-05T13:13:22.557694Z","shell.execute_reply":"2024-01-05T13:13:22.556902Z","shell.execute_reply.started":"2024-01-05T13:13:22.528760Z"},"trusted":true},"outputs":[],"source":["config = LoraConfig(\n","    r=2,\n","    lora_alpha=32,\n","    target_modules=get_last_layer_linears(model),\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Redefine the model with Lora config\n","model = get_peft_model(model, config)"]},{"cell_type":"markdown","metadata":{},"source":["Step 4.\n","- Define generation_config: this is for the result generation with parameters: max_new_tokens, temperature, top_p, etc.."]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:13:40.908013Z","iopub.status.busy":"2024-01-05T13:13:40.907268Z","iopub.status.idle":"2024-01-05T13:13:40.913452Z","shell.execute_reply":"2024-01-05T13:13:40.912495Z","shell.execute_reply.started":"2024-01-05T13:13:40.907976Z"},"trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","# max_new_tokens is limited length of the answer\n","generation_config.max_new_tokens = 100\n","# low temperature (0.1) for more predictable/coherent text\n","# high temperature (0.9) for more creative/unpredictable text\n","generation_config.temperature = 0.9\n","# top_p = 0.7 means the next word must have at least 70% chance to appear\n","generation_config.top_p = 0.7\n","generation_config.do_sample = True\n","generation_config.num_return_sequences = 1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{},"source":["\n","Step 5.\n","- Test pretrained model by defining a prompt with a question-answer pair. \n","- Define encoding = tokenizer(prompt)\n","- Generate output = model.generate(encoding)"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:13:44.194555Z","iopub.status.busy":"2024-01-05T13:13:44.193754Z","iopub.status.idle":"2024-01-05T13:13:44.200839Z","shell.execute_reply":"2024-01-05T13:13:44.199853Z","shell.execute_reply.started":"2024-01-05T13:13:44.194521Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Going on my first red eye, international flight soon. I’ve only ever done short domestic flights. And I’ve never used the bathroom on a plane. I heard someone say that if you’re in a window seat you should try and use the bathroom when the person sitting next to you gets up to use the bathroom. But other than that, if there any general etiquette that I should be aware of when in the window seat? Especially when it comes to possibly having to wake someone up to use the bathroom.. Answer:'"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["prompt = df[\"Question\"].values[10] + \". Answer: \".strip()\n","prompt"]},{"cell_type":"markdown","metadata":{},"source":["We will use this prompt to see the answer of the pretrained model."]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:13:50.401517Z","iopub.status.busy":"2024-01-05T13:13:50.401102Z","iopub.status.idle":"2024-01-05T13:14:07.448731Z","shell.execute_reply":"2024-01-05T13:14:07.447771Z","shell.execute_reply.started":"2024-01-05T13:13:50.401483Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Going on my first red eye, international flight soon. I’ve only ever done short domestic flights. And I’ve never used the bathroom on a plane. I heard someone say that if you’re in a window seat you should try and use the bathroom when the person sitting next to you gets up to use the bathroom. But other than that, if there any general etiquette that I should be aware of when in the window seat? Especially when it comes to possibly having to wake someone up to use the bathroom.. Answer: I can understand your concern. When it comes to using the bathroom on a plane, it's important to be mindful of your fellow passengers and follow some basic etiquette. Here are some tips that may be helpful:\n","\n","1. Use the bathroom before the flight: It's a good idea to use the bathroom before the flight, especially if you know you'll be in a window seat. This way, you can avoid having to use the bathroom during\n","CPU times: user 16.4 s, sys: 600 ms, total: 17 s\n","Wall time: 17 s\n"]}],"source":["%%time\n","device = \"cuda\"\n","\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","with torch.no_grad():\n","    outputs = model.generate(\n","        input_ids = encoding.input_ids,\n","        attention_mask = encoding.attention_mask,\n","        generation_config = generation_config\n","    )\n","\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":["The generated answer is \"It's a good idea to use the bathroom before the flight\". We will see the question again after fine-tuning.\n","\n","Step 6.\n","- Define data for our LLM. Note that all question-answer pairs will be tokenized in this step."]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:04:09.715035Z","iopub.status.busy":"2024-01-05T13:04:09.714761Z","iopub.status.idle":"2024-01-05T13:04:09.723272Z","shell.execute_reply":"2024-01-05T13:04:09.722333Z","shell.execute_reply.started":"2024-01-05T13:04:09.715012Z"},"trusted":true},"outputs":[],"source":["data = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:15:28.890497Z","iopub.status.busy":"2024-01-05T13:15:28.889658Z","iopub.status.idle":"2024-01-05T13:15:28.953429Z","shell.execute_reply":"2024-01-05T13:15:28.952590Z","shell.execute_reply.started":"2024-01-05T13:15:28.890461Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d855711c592249318ae045cf694becc2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/21 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}],"source":["def generate_prompt(data_point):\n","    return f\"\"\"\n","            {data_point[\"Question\"]}. \n","            Answer: {data_point[\"Answer\"]}\n","            \"\"\".strip()\n","\n","\n","def generate_and_tokenize_prompt(data_point):\n","    full_prompt = generate_prompt(data_point)\n","    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n","    return tokenized_full_prompt\n","\n","data = data.shuffle().map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{},"source":["\n","Step 7.\n","- Define training_args with some parameters such as output_dir, learning rate, optimizer, ...\n","- Define trainer = transformer(model, data, training_args)\n","- Fine tune the model by calling trainer.train()"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:15:49.515980Z","iopub.status.busy":"2024-01-05T13:15:49.515043Z","iopub.status.idle":"2024-01-05T13:17:02.700920Z","shell.execute_reply":"2024-01-05T13:17:02.700101Z","shell.execute_reply.started":"2024-01-05T13:15:49.515945Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5/5 00:59, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=5, training_loss=2.7560375213623045, metrics={'train_runtime': 72.7525, 'train_samples_per_second': 0.289, 'train_steps_per_second': 0.069, 'total_flos': 364360434278400.0, 'train_loss': 2.7560375213623045, 'epoch': 0.95})"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["training_args = transformers.TrainingArguments(\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    num_train_epochs=1,\n","    learning_rate=1e-4,\n","    fp16=True,\n","    output_dir=\"finetune_reddit\",\n","    optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.01,\n","    report_to=\"none\"\n",")\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=data,\n","    args=training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","model.config.use_cache = False\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["Step 8.\n","- Evaluate\n","\n","Because the size of the data is small, we can ommit the test step. But the pseudo-codes below show how it is done for ROUGE metric."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install evaluate\n","# import evaluate\n","\n","# rouge = evaluate.load('rouge')\n","\n","# original_response = append all generated answers with the original model\n","# finetuned_response = append all generated answers with the fine-tuned model\n","# human_response = append all answers in Anwer\n","\n","# original_model_results = rouge.compute(\n","#     predictions=original_response,\n","#     references=human_response,\n","#     use_aggregator=True,\n","#     use_stemmer=True,\n","# )\n","\n","# finetuned_model_results = rouge.compute(\n","#     predictions=finedtune_response,\n","#     references=human_response,\n","#     use_aggregator=True,\n","#     use_stemmer=True,\n","# )\n","\n","# print('ORIGINAL MODEL:')\n","# print(original_model_results)\n","# print('FINE-TUNED MODEL:')\n","# print(finetuned_model_results)"]},{"cell_type":"markdown","metadata":{},"source":["We come the final step for fine-tuning.\n","\n","Step 9.\n","- Save the pretrained model."]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:18:39.754026Z","iopub.status.busy":"2024-01-05T13:18:39.753637Z","iopub.status.idle":"2024-01-05T13:18:39.810306Z","shell.execute_reply":"2024-01-05T13:18:39.809544Z","shell.execute_reply.started":"2024-01-05T13:18:39.753996Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"trained-model\")"]},{"cell_type":"markdown","metadata":{},"source":["Next step is about how to load and use a fine-tuned model.\n","\n","Step 1.\n","- Load model: to do this, we need the fine-tuned file and we define our model again like the Step 1 in the fine-tuning process."]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:18:43.028865Z","iopub.status.busy":"2024-01-05T13:18:43.028470Z","iopub.status.idle":"2024-01-05T13:18:53.816105Z","shell.execute_reply":"2024-01-05T13:18:53.815261Z","shell.execute_reply.started":"2024-01-05T13:18:43.028834Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1af39b169ad045c0ab087201dfc005c7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["PEFT_MODEL = \"/kaggle/working/trained-model\"\n","\n","config = PeftConfig.from_pretrained(PEFT_MODEL)\n","model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path,\n","    return_dict=True,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True\n",")\n","\n","tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = PeftModel.from_pretrained(model, PEFT_MODEL)"]},{"cell_type":"markdown","metadata":{},"source":["Step 2.\n","- Define generation config: this is also similar to the corresponding step in the fine-tuning process"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:18:57.685368Z","iopub.status.busy":"2024-01-05T13:18:57.684982Z","iopub.status.idle":"2024-01-05T13:18:57.691129Z","shell.execute_reply":"2024-01-05T13:18:57.689910Z","shell.execute_reply.started":"2024-01-05T13:18:57.685325Z"},"trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 100\n","generation_config.do_sample = True\n","generation_config.temperature = 0.9\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{},"source":["Step 3.\n","- Test with a prompt to see the result."]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T13:19:00.712162Z","iopub.status.busy":"2024-01-05T13:19:00.711189Z","iopub.status.idle":"2024-01-05T13:19:13.273496Z","shell.execute_reply":"2024-01-05T13:19:13.272523Z","shell.execute_reply.started":"2024-01-05T13:19:00.712126Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Going on my first red eye, international flight soon. I’ve only ever done short domestic flights. And I’ve never used the bathroom on a plane. I heard someone say that if you’re in a window seat you should try and use the bathroom when the person sitting next to you gets up to use the bathroom. But other than that, if there any general etiquette that I should be aware of when in the window seat? Especially when it comes to possibly having to wake someone up to use the bathroom.. Answer: It's always a good idea to be mindful of your fellow passengers when using the bathroom on a flight, especially if you're in a window seat. Here are some general etiquette tips to keep in mind:\n","\n","1. Be considerate of the person sitting next to you: If you need to use the bathroom, try to wait until the person sitting next to you is not sleeping or has their seatbelt on. You can usually tell if someone is\n"]}],"source":["prompt = prompt = df[\"Question\"].values[10] + \". Answer: \".strip()\n","\n","device = \"cuda\"\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","with torch.inference_mode():\n","  outputs = model.generate(\n","      input_ids = encoding.input_ids,\n","      attention_mask = encoding.attention_mask,\n","      generation_config = generation_config\n","  )\n","\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4260151,"sourceId":7337918,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":3097,"sourceId":4302,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
