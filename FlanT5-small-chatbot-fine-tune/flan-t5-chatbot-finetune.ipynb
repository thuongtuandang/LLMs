{"cells":[{"cell_type":"markdown","metadata":{},"source":["This is a kaggle notebook to fine-tune Flan T5 small for a travel chatbot. The fine-tuning process is much faster than Llama 2, but the model is not very intelligent, and it may take a lot of time to fine-tune."]},{"cell_type":"code","execution_count":53,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-05T22:06:38.560197Z","iopub.status.busy":"2024-01-05T22:06:38.559233Z","iopub.status.idle":"2024-01-05T22:06:38.568342Z","shell.execute_reply":"2024-01-05T22:06:38.567432Z","shell.execute_reply.started":"2024-01-05T22:06:38.560160Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/reddit-more/reddit_more.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:06:38.571307Z","iopub.status.busy":"2024-01-05T22:06:38.570695Z","iopub.status.idle":"2024-01-05T22:09:01.598733Z","shell.execute_reply":"2024-01-05T22:09:01.597524Z","shell.execute_reply.started":"2024-01-05T22:06:38.571246Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!pip install -qqq bitsandbytes\n","!pip install -qqq torch\n","!pip install  -qqq -U git+https://github.com/huggingface/transformers\n","!pip install -qqq -U git+https://github.com/huggingface/peft\n","!pip install -qqq -U git+https://github.com/huggingface/accelerate\n","!pip install -qqq datasets\n","!pip install -qqq loralib\n","!pip install -qqq einops"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:01.600416Z","iopub.status.busy":"2024-01-05T22:09:01.600100Z","iopub.status.idle":"2024-01-05T22:09:01.607225Z","shell.execute_reply":"2024-01-05T22:09:01.606326Z","shell.execute_reply.started":"2024-01-05T22:09:01.600389Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","from pprint import pprint\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import load_dataset, Dataset\n","from huggingface_hub import notebook_login\n","\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:01.608729Z","iopub.status.busy":"2024-01-05T22:09:01.608416Z","iopub.status.idle":"2024-01-05T22:09:01.617513Z","shell.execute_reply":"2024-01-05T22:09:01.616546Z","shell.execute_reply.started":"2024-01-05T22:09:01.608704Z"},"trusted":true},"outputs":[],"source":["MODEL_NAME = 'google/flan-t5-small'"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:01.621334Z","iopub.status.busy":"2024-01-05T22:09:01.620958Z","iopub.status.idle":"2024-01-05T22:09:03.332736Z","shell.execute_reply":"2024-01-05T22:09:03.331706Z","shell.execute_reply.started":"2024-01-05T22:09:01.621300Z"},"trusted":true},"outputs":[],"source":["bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","t5_model = AutoModelForSeq2SeqLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config\n",")\n","\n","# PEFT wrapper the model for training / fine-tuning\n","t5_model = prepare_model_for_kbit_training(t5_model)\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{},"source":["We will perform the chatbot before fine-tuning."]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:03.334265Z","iopub.status.busy":"2024-01-05T22:09:03.333969Z","iopub.status.idle":"2024-01-05T22:09:03.340119Z","shell.execute_reply":"2024-01-05T22:09:03.339199Z","shell.execute_reply.started":"2024-01-05T22:09:03.334231Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\"\n","\n","def chat_with_bot(model, input_text):\n","    input_text = \"chatbot: \" + input_text + \" </s>\"\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n","\n","    # Generate bot's response\n","    output = model.generate(input_ids=input_ids, max_length=200, num_return_sequences=1)\n","    \n","    # Decode and return the response\n","    bot_response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return bot_response"]},{"cell_type":"markdown","metadata":{},"source":["Question from reddit: I am traveling to Japan in the beginning of May with 5 other friends. We're visiting Tokyo, Osaka and Kyoto for two weeks. What are some travel hacks that you guys could recommend or things you would have done different when you travelled to Japan.   "]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:03.341734Z","iopub.status.busy":"2024-01-05T22:09:03.341407Z","iopub.status.idle":"2024-01-05T22:09:37.972649Z","shell.execute_reply":"2024-01-05T22:09:37.971831Z","shell.execute_reply.started":"2024-01-05T22:09:03.341700Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["T5 Chatbot: Hi there! How can I help you?\n"]},{"name":"stdout","output_type":"stream","text":["You:  I am travelling to Japan. Are there some travel hacks you recommend?\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: no\n"]},{"name":"stdout","output_type":"stream","text":["You:  exit\n"]}],"source":["print(\"T5 Chatbot: Hi there! How can I help you?\")\n","user_input = input(\"You: \")\n","while user_input.lower() not in ['exit', 'quit']:\n","    response = chat_with_bot(t5_model, user_input)\n","    print(\"Chatbot:\", response)\n","    user_input = input(\"You: \")"]},{"cell_type":"markdown","metadata":{},"source":["We can fine-tune this and compare the answer. But first, we can take a look on the dataset."]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:37.973941Z","iopub.status.busy":"2024-01-05T22:09:37.973676Z","iopub.status.idle":"2024-01-05T22:09:38.125485Z","shell.execute_reply":"2024-01-05T22:09:38.124484Z","shell.execute_reply.started":"2024-01-05T22:09:37.973918Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question    119\n","Answer        0\n","dtype: int64\n","Question    0\n","Answer      0\n","dtype: int64\n"]}],"source":["df = pd.read_csv('/kaggle/input/reddit-more/reddit_more.csv')\n","# Remove NaN\n","print(df.isna().sum())\n","df = df.dropna()\n","print(df.isna().sum())\n","# We will fine-tune on a small subset\n","df = df.iloc[100:201]\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.126886Z","iopub.status.busy":"2024-01-05T22:09:38.126597Z","iopub.status.idle":"2024-01-05T22:09:38.133185Z","shell.execute_reply":"2024-01-05T22:09:38.132127Z","shell.execute_reply.started":"2024-01-05T22:09:38.126860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index([56], dtype='int64')\n"]}],"source":["# Search for the question we asked chatbot\n","search_text = 'visiting Tokyo, Osaka and Kyoto'\n","indices = df[df['Question'].str.contains(search_text)].index\n","print(indices)"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.134703Z","iopub.status.busy":"2024-01-05T22:09:38.134428Z","iopub.status.idle":"2024-01-05T22:09:38.146131Z","shell.execute_reply":"2024-01-05T22:09:38.145050Z","shell.execute_reply.started":"2024-01-05T22:09:38.134677Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"Hello m8s,  \\n\\n\\nI am traveling to Japan in the beginning of May with 5 other friends. We're visiting Tokyo, Osaka and Kyoto for two weeks. What are some travel hacks that you guys could recommend or things you would have done different when you travelled to Japan.   \\n\\n\\nThank you very much in advance any advice is greatly appreciated :)\""]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["# This is the question we asked chatbot\n","df[\"Question\"].values[56]"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.147691Z","iopub.status.busy":"2024-01-05T22:09:38.147417Z","iopub.status.idle":"2024-01-05T22:09:38.156607Z","shell.execute_reply":"2024-01-05T22:09:38.155606Z","shell.execute_reply.started":"2024-01-05T22:09:38.147668Z"},"trusted":true},"outputs":[],"source":["# Remove \\n\n","df['Question'] = df['Question'].str.replace('\\n', '')\n","df['Answer'] = df['Answer'].str.replace('\\n', '')"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.158204Z","iopub.status.busy":"2024-01-05T22:09:38.157852Z","iopub.status.idle":"2024-01-05T22:09:38.168116Z","shell.execute_reply":"2024-01-05T22:09:38.167334Z","shell.execute_reply.started":"2024-01-05T22:09:38.158176Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"Hello m8s,  I am traveling to Japan in the beginning of May with 5 other friends. We're visiting Tokyo, Osaka and Kyoto for two weeks. What are some travel hacks that you guys could recommend or things you would have done different when you travelled to Japan.   Thank you very much in advance any advice is greatly appreciated :)\""]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["# Check\n","df[\"Question\"].values[56]"]},{"cell_type":"markdown","metadata":{},"source":["We will take a closer look to the model parameters and see how many parameters we will need to fine-tune the last layer."]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.169668Z","iopub.status.busy":"2024-01-05T22:09:38.169345Z","iopub.status.idle":"2024-01-05T22:09:38.183074Z","shell.execute_reply":"2024-01-05T22:09:38.182228Z","shell.execute_reply.started":"2024-01-05T22:09:38.169640Z"},"trusted":true},"outputs":[],"source":["import re\n","# This is to get the numer of layers of our LLM\n","def get_num_layers(model):\n","    # We first define a set\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        # Name is of this form: model.layers.2.post_attention_layernorm.weight\n","        # The number 2 means the index of the post attention layer norm\n","        # We use regular expression to parse the number 2\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    # The number of layers is exactly maximum value of the set numbers\n","    return max(numbers)\n","\n","# This is to get the number of parameters of our LLM\n","def get_num_params(model):\n","    num_params = 0\n","    for _, param in model.named_parameters():\n","        num_params += param.numel()\n","    return num_params\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    return names\n","\n","def get_n_last_layer_params(model):\n","    last_layer_params = 0\n","\n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                last_layer_params += sum(p.numel() for p in module.parameters())\n","\n","    return last_layer_params"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.187400Z","iopub.status.busy":"2024-01-05T22:09:38.187051Z","iopub.status.idle":"2024-01-05T22:09:38.207178Z","shell.execute_reply":"2024-01-05T22:09:38.206217Z","shell.execute_reply.started":"2024-01-05T22:09:38.187374Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["7\n","59135360\n","['decoder.block.7.layer.0.SelfAttention.q', 'decoder.block.7.layer.0.SelfAttention.k', 'decoder.block.7.layer.0.SelfAttention.v', 'decoder.block.7.layer.0.SelfAttention.o', 'decoder.block.7.layer.1.EncDecAttention.q', 'decoder.block.7.layer.1.EncDecAttention.k', 'decoder.block.7.layer.1.EncDecAttention.v', 'decoder.block.7.layer.1.EncDecAttention.o', 'decoder.block.7.layer.2.DenseReluDense.wi_0', 'decoder.block.7.layer.2.DenseReluDense.wi_1', 'decoder.block.7.layer.2.DenseReluDense.wo']\n","1835008\n"]}],"source":["# Total number of layers and parameters\n","print(get_num_layers(t5_model))\n","# Total number of named params\n","print(get_num_params(t5_model))\n","# Parameter names of the last layer\n","print(get_last_layer_linears(t5_model))\n","# Total number of parameters in the last layer\n","print(get_n_last_layer_params(t5_model))"]},{"cell_type":"markdown","metadata":{},"source":["So we will fine-tune 1.8M parameters. We will next config parameters for Lora."]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.208813Z","iopub.status.busy":"2024-01-05T22:09:38.208457Z","iopub.status.idle":"2024-01-05T22:09:38.238892Z","shell.execute_reply":"2024-01-05T22:09:38.238189Z","shell.execute_reply.started":"2024-01-05T22:09:38.208787Z"},"trusted":true},"outputs":[],"source":["from peft import TaskType\n","\n","# device = torch.device('cuda:1')\n","# Define lora config\n","lora_config = LoraConfig(\n","    r=8, # Rank\n","    lora_alpha=32,\n","    target_modules=get_last_layer_linears(t5_model),\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",")\n","t5_model = get_peft_model(t5_model, lora_config)"]},{"cell_type":"markdown","metadata":{},"source":["And define the data for the fine-tuning with question-answer pairs."]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.240330Z","iopub.status.busy":"2024-01-05T22:09:38.239977Z","iopub.status.idle":"2024-01-05T22:09:38.249185Z","shell.execute_reply":"2024-01-05T22:09:38.248262Z","shell.execute_reply.started":"2024-01-05T22:09:38.240292Z"},"trusted":true},"outputs":[],"source":["# Define data\n","data = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.251112Z","iopub.status.busy":"2024-01-05T22:09:38.250789Z","iopub.status.idle":"2024-01-05T22:09:38.405894Z","shell.execute_reply":"2024-01-05T22:09:38.405093Z","shell.execute_reply.started":"2024-01-05T22:09:38.251081Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5043739024c34008b292f532e7250b10","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/101 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def generate_prompt(data_point):\n","    return f\"\"\"\n","            {data_point[\"Question\"]}. \n","            Answer: {data_point[\"Answer\"]}\n","            \"\"\".strip()\n","\n","\n","def generate_and_tokenize_prompt(data_point):\n","    full_prompt = generate_prompt(data_point)\n","    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n","    return tokenized_full_prompt\n","\n","data = data.shuffle().map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{},"source":["Define training arguments, trainer and train."]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:09:38.407527Z","iopub.status.busy":"2024-01-05T22:09:38.407133Z","iopub.status.idle":"2024-01-05T22:09:58.895212Z","shell.execute_reply":"2024-01-05T22:09:58.894161Z","shell.execute_reply.started":"2024-01-05T22:09:38.407491Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25/25 00:19, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=25, training_loss=0.0, metrics={'train_runtime': 20.0806, 'train_samples_per_second': 5.03, 'train_steps_per_second': 1.245, 'total_flos': 8859182008320.0, 'train_loss': 0.0, 'epoch': 0.99})"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["output_dir = 'finetune-t5'\n","\n","training_args = transformers.TrainingArguments(\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    num_train_epochs=1,\n","    learning_rate=1e-3,\n","    fp16=True,\n","    output_dir=output_dir,\n","    optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.01,\n","    report_to=\"none\"\n",")\n","\n","trainer = transformers.Trainer(\n","    model=t5_model,\n","    train_dataset=data,\n","    args=training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","t5_model.config.use_cache = False\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-05T22:10:25.440909Z","iopub.status.busy":"2024-01-05T22:10:25.440531Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["T5 Chatbot: Hi there! How can I help you?\n"]},{"name":"stdout","output_type":"stream","text":["You:  I am travelling to Japan. Are there some travel hacks you recommend?\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: if you are a traveler, you can use a travel hack\n"]}],"source":["print(\"T5 Chatbot: Hi there! How can I help you?\")\n","user_input = input(\"You: \")\n","while user_input.lower() not in ['exit', 'quit']:\n","    response = chat_with_bot(t5_model, user_input)\n","    print(\"Chatbot:\", response)\n","    user_input = input(\"You: \")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4260151,"sourceId":7337918,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
